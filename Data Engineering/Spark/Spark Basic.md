# Spark
- 인메모리 기반의 빅데이터 처리 플랫폼

## Spark & Hadoop
- 하둡의 맵리듀스는 맵리듀스 잡의 결과를 HDFS에 저장한 후 이를 다시 불러와 사용하는 방식
- 이때, DISK I/O로 인해 느린 속도
- 스파크는 인메로리 엔진으로 하둡 맵리듀스보다 빠르게 동작
- 스파크는 하둡 생태계 일부를 대체할 수 있다.

## Spark 사용 경우
- 스파크는 일괄 분석을 염두에 두고 설계했기 때문에 공유된 데이터를 비동기적으로 갱신하는 연산(온라인 트랜잭션 처리 등)에는 적합하지 않다.
    - 실시간 데이터를 처리하는 스파크 스트리밍은 단순히 타임 윈도로 분할한 스트림 데이터에 일괄 처리를 적용한 것이다.
- 또 스파크는 잡과 태스크를 시작하는 데 상당한 시간을 소모하기 때문에 대량의 데이터를 처리하는 작업이 아니라면 굳이 스파크를 사용할 필요가 없다.
    - 소량의 데이터를 처리할 때는 스파크 같은 분산 시스템보다 간단한 관계형 데이터베이스나 잘 짜인 스크립트가 훨씬 더 빠르다.

## Spark 구성 컴포넌트
- Spark Core, Spark SQL, Spark Streaming, Spark GraphX, Spark MLlib

### 1. Spark Core
- 스파크 잡과 다른 스파크 컴포넌트에 필요한 기본 기능 제공
- RDD(Resilient Distributed Dataset) 제공
    - 분산 데이터 컬렉션(데이터셋)을 추상화한 객체로 데이터셋에 적용할 수 있는 연산 및 변환 메서드를 함께 제공
    - 노드에 장애가 발생해도 데이터셋을 재구성할 수 있는 **복원성**을 갖춤
- HDFS, GlusterFS, AWS S3 등 다양한 파일 시스템에 접근 가능
- 공유 변수와 누적 변수를 사용해 컴퓨팅 노드 간에 정보를 공유할 수 있다.
- 네트워킹, 보안, 스케쥴링 및 데이터 셔플링 등 기본 기능 구현

### 2. Spark SQL
- 스파크와 HiveQL이 지원하는 SQL을 사용해 대규모 분산 정형 데이터를 다를 수 있는 기능 제공
- JSON, Parquet, RDB 테이블, Hive 테이블 등 다양한 정형 데이터를 읽고 쓰는 데도 Spark SQL을 사용할 수 있다.

<br>

- Spark SQL은 DataFrame과 Dataset에 적용된 연산을 일정 시점에 RDD 연산으로 변환해 일반 스파크 잡으로 실행한다.
- Spark SQL은 Catalyst라는 쿼리 최적화 프레임워크를 제공, 사용자가 직접 정의한 최적화 규칙을 적용해 프레임워크를 확장할 수도 있다.
- BI 도구 등 외부 시스템과 스파크를 연동할 수 있는 Apache Thrift 서버 제공
    - 외부 시스템은 기존 JDBC 및 ODBC 프로토콜을 이용해 Spark SQL 쿼리를 실행할 수 있다.

### 3. Spark Streaming
- 다양한 데이터 소스에서 유입되는 실시간 스트리밍 데이터를 처리하는 프레임워크
- 지원 스트리밍 소스 : HDFS, Kafka, Flume, Twitter, ZeroMQ, 커스텀 소스
- 장애가 발생하면 연산 결과를 자동 복구
- **이산 스트림(Discretized Stream, DStream)** 방식으로 스트리밍 데이터를 표현하는데, 가장 마지막 타임 윈도 안에 유입된 데이터를 RDD로 구성해 주기적으로 생성
- Spark Streaming과 다른 스파크 컴포넌트를 단일 프로그램에서 사용해 실시간 처리 연산과 머신러닝 작업, SQL 연산, 그래프 연산 등을 통합할 수도 있다.
- Spark 2.0에서는 정형 스트리밍 API를 도입해 마치 일괄 처리 프로그램을 구현하는 것처럼 스트리밍 프로그램을 구현할 수 있다.

### 4. Spark MLlib
- 머신러닝 알고리즘 라이브러리
- RDD 또는 DataFrame의 데이터셋을 변환하는 머신러닝 모델을 구현할 수 있다.

### 5. Spark GraphX
- **그래프 RDD(EdgeRDD 및 VertexRDD)** 형태의 그래프 구조를 만들 수 있는 다양한 기능 제공
- 그래프 알고리즘 구현
- Giraph(하둡에서 그래프 알고리즘을 실행할 수 있도록 지원하는 프로젝트)에 구현된 대규모 그래프 처리 및 메시지 전달 API인 Pregel도 동일하게 제공

## Spark 실행 과정
- **데이터 지역성**을 최대한 달성하려고 파일의 각 블록이 저장된 위치를 하둡에게 요형한 후, 모든 블록을 클러스터 노드의 RAM 메모리로 전송
- 전송이 왼료되면 스파크 셸에서 RAM에 저장된 각 블록(**파티션**)을 참조할 수 있다.

<br>

- RDD API를 사용해 RDD의 컬렉션을 필터링하고, 사용자 정의 함수로 컬렉션을 매핑하고, 누적 값 하나로 리듀스하고, 두 RDD를 서로 빼거나 교차하거나 결합하는 등 다양한 작업을 실행할 수 있다.

## Spark Shuffle
- 스파크에서 연산은 단일 파티션에서 작동   
- reduceByKey와 같이 특정 키에 매핑된 모든 값에 대한 연산을 수행하기 위해서는 파티션에 흩어진 특정키에 해당하는 값을 하나의 파티션으로 모아 줄 필요가 있음  
- 모든 키에 대한 모든 값을 찾기 위해 모든 파티션을 탐색하고, 해당하는 값들을 하나의 파티션으로 옮겨오는 과정을 셔플이라고 부른다.  
- 디스크 IO 또는 네트워크 IO가 발생함으로 비용이 매우 비싼 작업  

ex)   
- filter : 각 파티션에 있는 하나의 튜플에 대해 조건을 탐색하면 됨으로 셔플 발생 x  
- reduceByKey : 연산을 시작하기 위해서는 우선적으로 모든 파티션에 분산되어 있는 특정 키 값을 수집해야함으로 셔플 발생 
- 셔플이 발생하는 함수들 :
 - subtractByKey
 - groupBy
 - foldByKey
 - reduceByKey
 - aggregateByKey
 - transformations of a join of any type
 - distinct
 - cogroup

## Shared Variables
- 모든 노드에서 사용하기 위한 공유변수
- 공유변수로 지정한 값은 모든 노드에 중복되어 캐시된다.
- 반복적으로 사용해야하는 변수라면,
  스파크의 노드는 네트워크를 통해 통신 하기 때문에 모든 노드에 중복 캐시하는 시스템적 비용보다  
  네트워크 과정에서 발생하는 오버헤드 비용이 더 많이 발생하게 된다.

## Broadcast Variables
- 각 노드에 공유되는 읽기 전용 변수

## Accumulator
- 각 노드에 공유되는 누산기 함수